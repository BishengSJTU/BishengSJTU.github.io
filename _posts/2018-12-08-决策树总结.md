---
layout:     post
title:      决策树总结
subtitle:   对决策树涉及的概念和原理做一个总结
date:       2018-12-08
author:     Bisheng
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - Machine Learning
---

> “昨夜西风凋碧树。独上高楼，望尽天涯路。”
> 
>
> 

# 决策树提出的想法
我们知道，监督学习的任务就是给定N个数据的features$\cal{X}$和它们的labels$\cal{Y}$，它们之间存在映射函数$\cal{f:X}\mapsto\cal{Y}$
我们需要在hypothesis space$\cal{H}$中寻找函数$\cal{h}$去逼近$\cal{f}$
- 另一些模型————包括linear regression,logistic regression,svm,dnn等，它们的hypothesis space都是parameter space，
定义好模型后，如dnn确定好网络结构后，hypothesis space其实是整个映射函数空间的子集；
- 决策树模型的hypothesis space是整个泛函空间，即$\cal{H=}\lbrace\cal{h}\mid\cal{h:X}\mapsto\cal{Y}\rbrace$
因此，决策树的思想就是在泛函空间里寻找映射函数的估计。


# 什么是决策树
决策树
- 非叶子结点是数据的某一个dimension的feature所确定的指标，它们的作用是分割数据
- 每片叶子结点是一些Instance的集合，**每一个Instance会且只会落在其中一片叶子上**。因此，叶子结点的并集就是整个数据集。
如果一棵决策树有N个叶子结点，那么，这棵树将整个函数空间分割成了N份，**每一个子空间是一个与feature的坐标轴平行的超矩形空间**，每一个Instance落在其中的一个子空间内。
叶子节点用来做label prediction

# 决策树的要点
决策树的实现主要要解决三个问题：
## 1.决策树的分裂指标该如何选取，也就是非叶子节点该如何确定？

数据经过决策树的结点后，应该变得更加pure，也就是说，数据的不确定性应该减小，
也就是说，分割后数据的总的**条件熵**应该比分割前的**熵**有所减小。
这里，涉及到的概念就是熵，什么是熵？

### 熵
熵是来衡量一个变量/系统的**不确定性**指标，**熵越大，变量/系统的不确定性越小，变量/系统中含有的信息量越小**。
这里以离散变量为例，连续变量将求和改写为概率密度函数的积分即可。
可以证明，同等条件下，均匀分布的熵最大。
### 交叉熵与KL散度
交叉熵$$H(X,Y)$$和KL散度都可以用来衡量两个分布的接近程度。**交叉熵和KL散度越小，说明它们的分布越接近**。
KL散度衡量两个分布的接近程度更为准确，在进行logistic回归时本应用KL散度，但是数据的真实分布固定且已知，所以最小化KL散度等同于最小化它们的交叉熵。
### 条件熵
条件熵$$H(X\mid{Y})$$是指在变量Y分布已知的情况下，X的分布的熵
### 信息增益与信息增益率
$$I(X,Y)$$是指在X原来的熵减去在变量Y分布已知的情况下，X的分布的熵。就是说知道Y的分布会给X带来怎样的变化，信息增益也称作互信息。
信息增益率是信息增益$$I(X,Y)$$与特征Y分布熵的比值

介绍完这么多，怎么选择分裂指标呢？选择的分裂指标应该使得数据的信息增益/信息增益率最大，就是说选择feature Y，使得数据X的信息增益最大。这样的Y使得X的分布的不确定性最大化地减小。

## 2.如何做预测？
在选择完所有分裂指标后，这棵树的结构也就确定了。在预测时对于每一个Instance，让它通过这棵树，最终它会**唯一地落到其中一片叶子结点上**，根据这片叶子结点的大多数label确定我们的prediction


# 决策树的生成
ID3和C4.5一般用在离散数据上，**每一个特征最多使用一次**，如果一个非叶子节点选择的特征F有N个取值，那么它最多可能会把数据分裂到N棵子树中，**ID3和C4.5可能是一棵多叉树**。
直到信息增益（率）很小或没有特征可以选择为止。
如果我们优化的损失函数只是所有叶子结点的分布熵求和，那么往往会过拟合，因此会我们会引入正则项，将叶子结点树乘以正则系数加到损失函数中，不希望总的叶子数太多。
## ID3
- 结点分裂标准：信息增益

## C4.5
- 结点分裂标准：信息增益率

CART**既可以用在连续数据上，又可以用在离散数据上**，并且**每一个特征可以重复使用**，每个特征只有yes或no两种选择，因此**CART树肯定是一棵二叉树**。
- 连续特征：选择切分特征j和切分点s时，会将空间继续划分为$$R1(j,s)=\lbrace{x\mid{x^{(j)}}\leq{s}}\rbrace$$和$$R2(j,s)=\lbrace{x\mid{x^{(j)}}>{s}}\rbrace$$
- 离散特征：选择切分特征j和切分种类a时，会将空间继续划分为$$R1(j,s)=\lbrace{x\mid{x^{(j)}}{\neq}a}\rbrace$$和$$R2(j,s)=\lbrace{x\mid{x^{(j)}}=a}\rbrace$$

## CART(Classification And Regression Tree)
### 分类树
- 结点分裂标准：基尼系数。

### 回归树
- 结点分裂标准：最小二乘回归。
此时需要最小化预测值和实际值的最小二乘误差，预测值C1取所有属于R1(j,s)的label的平均值，C2取所有属于R2(j,s)的label的平均值

# 决策树的优缺点
- 优点：1.可以处理某些特征值缺失的情况；2.既可以处理连续特征又可以处理离散特征
- 缺点：1.处理特征的线性组合能力差；2.单棵决策树预测能力较差



<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>