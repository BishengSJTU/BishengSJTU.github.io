---
layout:     post
title:      决策树总结
subtitle:   对决策树的涉及的概念和原理做一个总结
date:       2018-12-08
author:     BS
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - Machine Learning
---

> “昨夜西风凋碧树。独上高楼，望尽天涯路。”
> 
> [我的博客](http://bishengsjtu.github.io)
>
> 

# 决策树提出的想法
我们知道，监督学习的任务就是给定N个数据的features$\cal{X}$和它们的labels$\cal{Y}$，它们之间存在映射函数$\cal{f:X}\mapsto\cal{Y}$
我们需要在hypothesis space$\cal{H}$中寻找函数$\cal{h}$去逼近$\cal{f}$
- 另一些模型————包括linear regression,logistic regression,svm,dnn等，它们的hypothesis space都是parameter space，
定义好模型后，如dnn确定好网络结构后，hypothesis space其实是整个映射函数空间的子集；
- 决策树模型的hypothesis space是整个泛函空间，即$\cal{H=}\lbrace\cal{h}\mid\cal{h:X}\mapsto\cal{Y}\rbrace$
因此，决策树的思想就是在泛函空间里寻找映射函数的估计。


# 什么是决策树
决策树
- 非叶子结点是数据的某一个dimension的feature所确定的指标，它们的作用是分割数据
- 每片叶子结点是一些Instance的集合，每一个Instance会且只会落在其中一片叶子上。因此，叶子结点的并集就是整个数据集。
如果一棵决策树有N个叶子结点，那么，这棵树将整个函数空间分割成了N份，每一个Instance落在其中的一个子空间内。
叶子节点用来做label prediction

# 决策树的实现细节
决策树的实现主要要解决三个问题：
## 1.决策树的分裂指标该如何选取，也就是非叶子节点该如何确定？

数据经过决策树的结点后，应该变得更加pure，也就是说，数据的不确定性应该减小，
也就是说，分割后数据的总的**条件熵**应该比分割前的**熵**有所减小。
这里，涉及到的概念就是熵，什么是熵？
### 熵
熵是来衡量一个变量/系统的不确定性指标，熵越大，变量/系统的不确定性越小，变量/系统中含有的信息量越小。
X的熵可以定义为
$$
H(X)=-\sum_{i=1}^n{p_i}log{p_i}
$$
可以证明，
$$
H(X)\leq-\sum_{i=1}^n \frac{1}{n} \quad log \quad \frac{1}{n}
$$

## 2.如何做预测？
## 3.树的结构应该是怎样的？


# 决策树的分类

# 决策树的优缺点


<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>